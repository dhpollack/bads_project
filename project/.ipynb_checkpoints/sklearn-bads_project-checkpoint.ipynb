{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadDataset(df, date_to_int = True):\n",
    "    # remove NA\n",
    "    df.fillna(-99, inplace = True)\n",
    "    # Convert Dates\n",
    "    df.order_date = pd.to_datetime(df.order_date, format='%Y-%m-%d')\n",
    "    df.account_creation_date = pd.to_datetime(df.account_creation_date, format='%Y-%m-%d')\n",
    "    df.deliverydate_estimated = pd.to_datetime(df.deliverydate_estimated, format='%Y-%m-%d')\n",
    "    df.deliverydate_actual = pd.to_datetime(df.deliverydate_actual, format='%Y-%m-%d')\n",
    "    # Create weekday dummy\n",
    "    df['x_order_date_is_weekday'] = df.order_date.dt.dayofweek < 5\n",
    "    if date_to_int:\n",
    "        epoch_date = pd.Timestamp(\"2013-01-01\")\n",
    "        df.order_date = (df.order_date - epoch_date).astype('timedelta64[D]').astype(int)\n",
    "        df.account_creation_date = (df.account_creation_date - epoch_date).astype('timedelta64[D]').astype(int)\n",
    "        df.deliverydate_estimated = (df.deliverydate_estimated - epoch_date).astype('timedelta64[D]').astype(int)\n",
    "        df.deliverydate_actual = (df.deliverydate_actual - epoch_date).astype('timedelta64[D]').astype(int)\n",
    "    # Convert Categories (factors in R lingo)\n",
    "    df.model = df.model.astype('category')\n",
    "    df.form_of_address = df.form_of_address.astype('category')\n",
    "    df.email_domain = df.email_domain.astype('category')\n",
    "    df.postcode_invoice = df.postcode_invoice.astype('category')\n",
    "    df.postcode_delivery = df.postcode_delivery.astype('category')\n",
    "    df.payment = df.payment.astype('category')\n",
    "    df.advertising_code = df.advertising_code.astype('category')\n",
    "    df.x_advertising_code_bin = df.x_advertising_code_bin.astype('category')\n",
    "    df.x_order_date_yearweek = df.x_order_date_yearweek.astype('category')\n",
    "    # Categorize _bin columns\n",
    "    cols = df.columns\n",
    "    cols_bin = cols[cols.str.contains(\"_bin\")].values.tolist()\n",
    "    for col_bin in cols_bin:\n",
    "        df[col_bin] = df[col_bin].astype('category')\n",
    "    return(df)\n",
    "\n",
    "def simple_oversample_idx(y):\n",
    "    y_idx_0 = np.where(y == 0)[0]\n",
    "    y_idx_1 = np.random.choice(np.where(y == 1)[0], size=y_idx_0.shape[0], replace=True)\n",
    "    ret_cust_idx = []\n",
    "    ret_cust_idx.extend(y[y_idx_0])\n",
    "    ret_cust_idx.extend(y[y_idx_1])\n",
    "    return(ret_cust_idx)\n",
    "\n",
    "\n",
    "def bads_costs(y_t, yhat, m = np.array([[3., 0.], [-10., 0.]])):\n",
    "    N = yhat.shape[0]\n",
    "    C = confusion_matrix(y_t, yhat)\n",
    "    return(np.multiply(C, m).sum() / N)\n",
    "\n",
    "def bads_scorer(y_t, yhat_prob, m = np.array([[3., 0.], [-10., 0.]])):\n",
    "    thresholds = np.linspace(0.01, 0.99)\n",
    "    costs = [bads_costs(y_t, yhat_prob[:,1] > threshold, m) for threshold in thresholds]\n",
    "    return(np.max(costs))\n",
    "\n",
    "def find_corr_features(df, threshold = 0.7):\n",
    "    cols = df.columns.values.tolist()\n",
    "    corr_mat = df.corr()\n",
    "    corr_items = np.where(np.abs(np.triu(corr_mat, k=1)) > threshold)\n",
    "    cols_removed = []\n",
    "    for corr_item in list(set([cols[max(item)] for item in zip(*corr_items)])):\n",
    "        cols_removed.append(corr_item)\n",
    "        cols.remove(corr_item)\n",
    "    print(\"Removing Columns:\", \", \".join(cols_removed))\n",
    "    return(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "#\n",
    "#  Variable Selection\n",
    "#\n",
    "#####################################\n",
    "######### Set Seed #########\n",
    "rs = 90049\n",
    "save_model = False\n",
    "\n",
    "######### Feature Selection #########\n",
    "manual_features_to_remove = [\"item_count\", \"x_order_date_num\"]\n",
    "feature_correlation_removal = False\n",
    "feature_correlation_threshold = 0.7\n",
    "automatic_feature_selection = False\n",
    "automatic_feature_threshold = 0.005\n",
    "\n",
    "######### Oversampling #########\n",
    "# non-standard package: http://contrib.scikit-learn.org/imbalanced-learn/index.html\n",
    "oversample_method = \"none\"\n",
    "\n",
    "######### Cross-Valdiation #########\n",
    "do_cv = False # this takes a long time\n",
    "cv_num_folds = 4\n",
    "cv_validation_frac = 0.15\n",
    "cv_rs_iters = 10\n",
    "cost_func = bads_costs # bads_costs, roc_auc_score\n",
    "score_func = bads_scorer # bads_scorer, roc_auc_score\n",
    "#cost_func = roc_auc_score # bads_costs, roc_auc_score\n",
    "#score_func = roc_auc_score # bads_scorer, roc_auc_score\n",
    "\n",
    "######### Model Selection #########\n",
    "model_to_use = \"rf\" # \"rf\" or \"gbc\" or \"linear\"\n",
    "\n",
    "if model_to_use == \"rf\":\n",
    "    # Random Forest Classifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(random_state=rs)\n",
    "    automatic_feature_selection_params = {'n_estimators': 250, 'verbose': 0, 'n_jobs': 3}\n",
    "    clf_default_params = {'min_samples_split': 2, 'n_estimators': 250, 'min_samples_leaf': 9, 'verbose': 0, 'n_jobs': 3}\n",
    "    cv_param_grid = {'n_estimators':[100, 250, 500], 'min_samples_split':[2, 4, 8], 'min_samples_leaf': [1, 3, 9], 'n_jobs': [3]}\n",
    "elif model_to_use == \"gbc\":\n",
    "    # Gradient Boosting Classifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    clf = GradientBoostingClassifier(random_state=rs)\n",
    "    automatic_feature_selection_params = {'n_estimators': 50, 'verbose': 1}\n",
    "    clf_default_params = {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'verbose': 1}\n",
    "    cv_param_grid = {'n_estimators':[50, 100, 250, 500], 'learning_rate':[0.05, 0.1, .25], 'max_depth': [3, 5, 9]}\n",
    "elif model_to_use == \"linear\":\n",
    "    # Logistic Regression Classifier\n",
    "    from sklearn import linear_model\n",
    "    clf = linear_model.LogisticRegression()\n",
    "    clf_default_params = {'penalty': 'l1'}\n",
    "    cv_param_grid = {'penalty':['l1', 'l2'], 'C': 2 ** np.linspace(-3, 5, 17), 'n_jobs': [3]}\n",
    "else:\n",
    "    print(\"Invalid Model Selected\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No oversampling...\n",
      "Starting automatic feature selection...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-f78048c5c84c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Automatic Feature Selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m X_train, X_train_cv, X_valid_cv, X_test = automagic_feature_selection(True, clf, automatic_feature_selection_params, \n\u001b[1;32m---> 15\u001b[1;33m                                                                       X_train, X_train_cv, X_valid_cv, X_test)\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_valid_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-201834203ac2>\u001b[0m in \u001b[0;36mautomagic_feature_selection\u001b[1;34m(automatic_feature_selection, clf, automatic_feature_selection_params, X_train, X_train_cv, X_valid_cv, X_test)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[0mimportant_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_importances_\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mautomatic_feature_threshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[0mimportant_features_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"return_customer\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimportant_features\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"High Importance Features:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\", \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportant_features_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"output/optimal_features.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimportant_features_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"%s\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\";\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "# Load and split training  and testing data\n",
    "X_train, y_train, X_test = create_datasets()\n",
    "\n",
    "# Create Validation Splits\n",
    "X_train_cv, X_valid_cv, y_train_cv, y_valid_cv = train_test_split(X_train, y_train, \n",
    "                                                                  test_size = cv_validation_frac, \n",
    "                                                                  stratify = y_train, \n",
    "                                                                  random_state = rs)\n",
    "\n",
    "# Oversample\n",
    "X_train, y_train, X_train_cv, y_train_cv = oversample(oversample_method, X_train, y_train, X_train_cv, y_train_cv)\n",
    "\n",
    "# Automatic Feature Selection\n",
    "X_train, X_train_cv, X_valid_cv, X_test = automagic_feature_selection(automatic_feature_selection, automatic_feature_threshold, \n",
    "                                                                      clf, automatic_feature_selection_params, \n",
    "                                                                      X_train, X_train_cv, X_valid_cv, X_test)\n",
    "print(X_train.shape, X_test.shape, X_train_cv.shape, X_valid_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the model\n",
    "\n",
    "# Cross-Validation\n",
    "if do_cv:\n",
    "    # this can take a LONG time\n",
    "    print(\"Searching for best parameters with CV search...\")\n",
    "    clf_cv = RandomizedSearchCV(clf, cv_param_grid, \n",
    "                             scoring = make_scorer(score_func, needs_proba=True), \n",
    "                             cv = cv_num_folds,\n",
    "                             n_iter = cv_rs_iters, \n",
    "                             random_state = rs, verbose = 1)\n",
    "    clf_cv.fit(X_train_cv, y_train_cv)\n",
    "    #clf_rf_cv.cv_results_\n",
    "    joblib.dump(clf_cv.cv_results_, 'output/clf_rf_cv.results.pkl')\n",
    "    print(\"Cross Valdiation Report:\")\n",
    "    print(clf_cv.best_params_)\n",
    "    print(\"Best Score:\", clf_cv.best_score_)\n",
    "    # Plot Expected ROI per Customer\n",
    "    plt.errorbar(range(cv_rs_iters), \n",
    "                 clf_cv.cv_results_[\"mean_test_score\"], \n",
    "                 yerr = clf_cv.cv_results_[\"std_test_score\"], \n",
    "                 fmt=\"o\")\n",
    "    plt.margins(0.03)\n",
    "    plt.show()\n",
    "\n",
    "    # Train and Validate a random forest classifier with the best parameters\n",
    "    yhat_valid_prob = clf_cv.predict_proba(X_valid_cv)\n",
    "    \n",
    "    params_star = clf_cv.best_params_\n",
    "    clf.set_params(**params_star)\n",
    "else:\n",
    "    clf.set_params(**clf_default_params)\n",
    "    clf.fit(X_train_cv, y_train_cv)\n",
    "    yhat_valid_prob = clf.predict_proba(X_valid_cv)\n",
    "    \n",
    "print(\"Validation Summary:\")\n",
    "print(\"Calculate Optimal Threshold\")\n",
    "thresholds = np.linspace(0.01, 0.99, 197)\n",
    "costs = [bads_costs(y_valid_cv, yhat_valid_prob[:,1] > threshold) for threshold in thresholds]\n",
    "threshold_star = thresholds[np.argmax(costs)]\n",
    "# Plot\n",
    "plt.plot(thresholds, costs)\n",
    "plt.show()\n",
    "print(\"Threshold:\", threshold_star)\n",
    "yhat_valid = yhat_valid_prob[:,1] > threshold_star\n",
    "print(\"Average ROI:\", cost_func(y_valid_cv, yhat_valid))\n",
    "print(\"ROC Score:\", roc_auc_score(y_valid_cv, yhat_valid_prob[:,1]))\n",
    "print(\"Validation Return Customers: {} of {} ({}%)\".format(np.sum(yhat_valid), \n",
    "                                                           len(yhat_valid), \n",
    "                                                           np.sum(yhat_valid)/len(yhat_valid)))\n",
    "print(confusion_matrix(y_valid_cv, yhat_valid))\n",
    "# Train model with all data and use on the Test set\n",
    "clf.fit(X_train, y_train)\n",
    "yhat_test_proba = clf.predict_proba(X_test)\n",
    "yhat_test = yhat_test_proba[:,1] > threshold_star\n",
    "np.savetxt(\"output/test_return_customer.csv\", yhat_test.astype(int), fmt='%i', delimiter=\";\")\n",
    "print(\"Testing Return Customers: {} of {} ({}%)\".format(np.sum(yhat_test), \n",
    "                                                        len(yhat_test), \n",
    "                                                        np.sum(yhat_test)/len(yhat_test)))\n",
    "\n",
    "if save_model:\n",
    "    joblib.dump(clf, 'output/model_final.pkl')\n",
    "    #clf_rf = joblib.load('output/model_final.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PCA Analysis of Results\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "num_PC = 5\n",
    "\n",
    "train_scaled = scale(X_train)\n",
    "pca = PCA(n_components=num_PC)\n",
    "pca.fit(train_scaled)\n",
    "train_rotated = pca.transform(train_scaled)\n",
    "df_train = pd.DataFrame(train_rotated)\n",
    "df_train[\"colors\"] = y_train\n",
    "sns.pairplot(df_train, hue = \"colors\", diag_kind=\"kde\", vars=range(num_PC))\n",
    "plt.show()\n",
    "test_scaled = scale(X_test)\n",
    "test_rotated = pca.transform(test_scaled)\n",
    "df_test = pd.DataFrame(test_rotated)\n",
    "df_test[\"colors\"] = yhat_test\n",
    "sns.pairplot(df_test, hue = \"colors\", diag_kind=\"kde\", vars=range(num_PC))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tests Below This Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Single Random Forest\n",
    "\n",
    "cost_func = roc_auc_score # bads_costs roc_auc_score\n",
    "\n",
    "clf_rf = RandomForestClassifier(n_estimators=4000)\n",
    "\n",
    "clf_rf.fit(X_train_cv, y_train_cv)\n",
    "yhat_valid = clf_rf.predict(X_valid_cv)\n",
    "print(\"Validation Summary:\")\n",
    "print(cost_func(y_valid_cv, yhat_valid))\n",
    "print(\"Validation: {} of {} ({}%)\".format(np.sum(yhat_valid), len(yhat_valid), np.sum(yhat_valid)/len(yhat_valid)))\n",
    "print(confusion_matrix(y_valid_cv, yhat_valid))\n",
    "\n",
    "clf_rf.fit(X_train, y_train)\n",
    "print(\"Test Results\")\n",
    "yhat = clf_rf.predict(X_test)\n",
    "print(\"{} of {}\".format(np.sum(yhat), len(yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gradient Boost Classifier (testing)\n",
    "\n",
    "cost_func = roc_auc_score # bads_costs roc_auc_score\n",
    "\n",
    "clf_gbc = GradientBoostingClassifier(learning_rate=0.05, n_estimators=500, random_state=1234, verbose=1)\n",
    "clf_gbc.fit(X_train_cv, y_train_cv)\n",
    "yhat_valid = clf_gbc.predict(X_valid_cv)\n",
    "print(\"Validation Summary:\")\n",
    "print(cost_func(y_valid_cv, yhat_valid))\n",
    "print(\"Validation: {} of {}\".format(np.sum(yhat_valid), len(yhat_valid)))\n",
    "print(confusion_matrix(y_valid_cv, yhat_valid))\n",
    "print(X_train.columns[np.where(clf_gbc.feature_importances_ > 0.005)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample Confusion Matrix\n",
    "\n",
    "a = np.array([1, 0, 0, 1, 1, 1, 0, 0, 0, 0])\n",
    "b = np.array([True, False, False, False, False, False, True, True, True, True])\n",
    "confusion_matrix(a,b), bads_costs(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(range(10), clf_cv.cv_results_[\"mean_test_score\"], yerr = clf_cv.cv_results_[\"std_test_score\"], fmt=\"o\")\n",
    "plt.margins(0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Results\n",
    "\n",
    "### Gradient Boosting: SMOTE+Tomek, 500 iterations, .1 eta, 5 max_depth, no decorrellation, no feature selection, no hyper-parameter search\n",
    "\n",
    "Time: ~45m\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.29\n",
    "0.772966722344\n",
    "Validation Return Customers: 1193 of 7783\n",
    "[[5532  785]\n",
    " [1058  408]]\n",
    "Testing Return Customers: 1478 of 12971\n",
    "\n",
    "### Gradient Boosting: SMOTE+Tomek, 50 iterations, .25 eta, 5 max_depth, no decorrellation, no feature selection, no hyper-parameter search\n",
    "\n",
    "Time: ~5m; iter: 50; training loss: 0.5494\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.23\n",
    "0.775664910703\n",
    "Validation Return Customers: 2239 of 7783\n",
    "[[4729 1588]\n",
    " [ 815  651]]\n",
    "Testing Return Customers: 3694 of 12971\n",
    "\n",
    "### Gradient Boosting: SMOTE+Tomek, 50 iterations, .25 eta, 12 max_depth, no decorrellation, no feature selection, no hyper-parameter search\n",
    "\n",
    "Time: 30.77m; iter: 50; training loss: 0.3084\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.25\n",
    "0.687909546447\n",
    "Validation Return Customers: 1613 of 7783\n",
    "[[5158 1159]\n",
    " [1012  454]]\n",
    "Testing Return Customers: did not complete\n",
    "\n",
    "### Gradient Boosting: No Oversampling, 50 iterations, .25 eta, 12 max_depth, no decorrellation, no feature selection, no hyper-parameter search\n",
    "\n",
    "Time: 23.24m; iter: 50; training loss: 0.4498\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.17\n",
    "0.67814467429\n",
    "Validation Return Customers: 2687 of 7783\n",
    "[[4326 1991]\n",
    " [ 770  696]]\n",
    "Testing Return Customers: 3321 of 12971\n",
    "\n",
    "### Gradient Boosting: No Oversampling, 50 iterations, .15 eta, 3 max_depth, no decorrellation, no feature selection, no hyper-parameter search\n",
    "\n",
    "Time: 1.20m; iter: 50; training loss: 0.9077\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.21\n",
    "0.812925607092\n",
    "Validation Return Customers: 2294 of 7783\n",
    "[[4709 1608]\n",
    " [ 780  686]]\n",
    "Testing Return Customers: 3924 of 12971\n",
    "\n",
    "### Gradient Boosting: No Oversampling, 50 iterations, .15 eta, 3 max_depth, decorrelation, feature selection, no hyper-parameter search\n",
    "\n",
    "Time: 4.51m; iter: 50; training loss: 0.9096\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.21\n",
    "0.824617756649\n",
    "Validation Return Customers: 2385 of 7783\n",
    "[[4646 1671]\n",
    " [ 752  714]]\n",
    "Testing Return Customers: 4003 of 12971\n",
    "\n",
    "### Gradient Boosting: No Oversampling, 100 iterations, .05 eta, 3 max_depth, decorrelation, feature selection, no hyper-parameter search\n",
    "\n",
    "Time: 7.99m; iter: 100; training loss: 0.9159\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.23\n",
    "0.809970448413\n",
    "Validation Return Customers: 1669 of 7783\n",
    "[[5188 1129]\n",
    " [ 926  540]]\n",
    "Testing Return Customers: 2815 of 12971\n",
    "\n",
    "---\n",
    "\n",
    "### RandomForest: No Oversampling, 500 trees, min sample split 4, min leaf size 9, decorr, gb feature selection, no hyper-parameter search\n",
    "\n",
    "Time: ~2m\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.23\n",
    "0.800848002056\n",
    "Validation Return Customers: 2178 of 7783\n",
    "[[4791 1526]\n",
    " [ 814  652]]\n",
    "Testing Return Customers: 3700 of 12971\n",
    "\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "### RandomForest: No Oversampling, All Bin to Cat, 500 trees, min sample split 4, min leaf size 9, no decorr, no feature selection, no hyper-parameter search\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.24\n",
    "Average ROI: 0.837851728125\n",
    "ROC Score: 0.662306189517\n",
    "Validation Return Customers: 1705 of 7783 (0.21906719773866118%)\n",
    "[[5177 1140]\n",
    " [ 901  565]]\n",
    "Testing Return Customers: 2801 of 12971 (0.21594325803715983%)\n",
    "\n",
    "\n",
    "### RandomForest: No Oversampling, All Bin to Cat, 500 trees, min sample split 4, min leaf size 9, decorr, rf feature selection, no hyper-parameter search\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.23\n",
    "Average ROI: 0.788513426699\n",
    "ROC Score: 0.647177725452\n",
    "Validation Return Customers: 2184 of 7783 (0.28061158936142877%)\n",
    "[[4779 1538]\n",
    " [ 820  646]]\n",
    "Testing Return Customers: 4028 of 12971 (0.31053889445686533%)\n",
    "\n",
    "### RandomForest: SMOTE Oversampling, All Bin to Cat, 500 trees, min sample split 4, min leaf size 9, decorr, rf feature selection, no hyper-parameter search\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.27\n",
    "Average ROI: 0.796479506617\n",
    "ROC Score: 0.64603845143\n",
    "Validation Return Customers: 2016 of 7783 (0.2590260824874727%)\n",
    "[[4913 1404]\n",
    " [ 854  612]]\n",
    "Testing Return Customers: 8841 of 12971 (0.6815974096060442%)\n",
    "\n",
    "### Logistic Regression: no Oversampling, L1 Penalty, C = 0.5, no decorr, no feature selection, rand hyper-parameter search\n",
    "\n",
    "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n",
    "Done  40 out of  40 | elapsed: 27.8min finished\n",
    "\n",
    "{'n_jobs': 3, 'C': 0.5, 'penalty': 'l1'}\n",
    "Best Score: 0.794607832022\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.23\n",
    "Average ROI: 0.783374020301\n",
    "ROC Score: 0.649006146605\n",
    "Validation Return Customers: 1998 of 7783 (0.25671334960812026%)\n",
    "[[4919 1398]\n",
    " [ 866  600]]\n",
    "Testing Return Customers: 3266 of 12971 (0.25179246010330736%)\n",
    "\n",
    "### Random Forest: no Oversampling, min_sample_leaf = 9, n_estimators = 250, min_samples_split = 2, no decorr, no feature selection, rand hyper-parameter search\n",
    "\n",
    "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n",
    "Done  40 out of  40 | elapsed: 71.8min finished\n",
    "Cross Valdiation Report:\n",
    "{'n_jobs': 3, 'min_samples_leaf': 9, 'n_estimators': 250, 'min_samples_split': 2}\n",
    "Best Score: 0.811478197773\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.235\n",
    "Average ROI: 0.833097777207\n",
    "ROC Score: 0.662557465822\n",
    "Validation Return Customers: 1856 of 7783 (0.23846845689322882%)\n",
    "[[5058 1259]\n",
    " [ 869  597]]\n",
    "Testing Return Customers: 3057 of 12971 (0.23567959293809268%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RandomizedSearchCV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = np.array([[3., 0.], [-10., 0.]])\n",
    "cm0 = confusion_matrix(train.return_customer, np.zeros(train.return_customer.count()))\n",
    "cm1 = confusion_matrix(train.return_customer, np.ones(train.return_customer.count()))\n",
    "costs = np.multiply(cm0, m)\n",
    "print(cm0);print(m);print(costs);print(costs.sum()/train.return_customer.count());print(np.multiply(cm1, m).sum()/train.return_customer.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-c8a22e881ccc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"help!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBADS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msayhello\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"A\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-c8a22e881ccc>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Hello world\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train_cv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_valid_cv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_train_cv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my_valid_cv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msayhello\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "class BADS(object):\n",
    "    def __init__(self):\n",
    "        self.test = \"Hello world\"\n",
    "\n",
    "    def sayhello(self, a):\n",
    "        print(self.test)\n",
    "        print(\"help!\")\n",
    "a = BADS()\n",
    "a.sayhello(\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
