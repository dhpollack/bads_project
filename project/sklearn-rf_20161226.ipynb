{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, scale\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "# non-standard package: http://contrib.scikit-learn.org/imbalanced-learn/index.html\n",
    "# https://www.jair.org/media/953/live-953-2037-jair.pdf\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadDataset(df):\n",
    "    # remove NA\n",
    "    df.fillna(-99999, inplace = True)\n",
    "    # Convert Dates\n",
    "    df.order_date = pd.to_datetime(df.order_date, format='%Y-%m-%d')\n",
    "    df.account_creation_date = pd.to_datetime(df.account_creation_date, format='%Y-%m-%d')\n",
    "    df.deliverydate_estimated = pd.to_datetime(df.deliverydate_estimated, format='%Y-%m-%d')\n",
    "    df.deliverydate_actual = pd.to_datetime(df.deliverydate_actual, format='%Y-%m-%d')\n",
    "    # Convert Categories\n",
    "    df.form_of_address = df.form_of_address.astype('category')\n",
    "    df.email_domain = df.email_domain.astype('category')\n",
    "    df.payment = df.payment.astype('category')\n",
    "    df.advertising_code = df.advertising_code.astype('category')\n",
    "    df.x_advertising_code_bin = df.x_advertising_code_bin.astype('category')\n",
    "    df.x_order_date_yearweek = df.x_order_date_yearweek.astype('category')\n",
    "    return(df)\n",
    "\n",
    "def bads_costs(y_t, yhat, m = np.array([[3., 0.], [-10., 0.]])):\n",
    "    N = yhat.shape[0]\n",
    "    C = confusion_matrix(y_t, yhat)\n",
    "    return(np.multiply(C, m).sum() / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((83918, 16), (12971, 16))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_full_feature_set = False\n",
    "\n",
    "features_to_use = [\n",
    "       'x_order_date_num',\n",
    "       #'form_of_address', 'title', 'email_domain',\n",
    "       'newsletter', 'model', 'payment',\n",
    "       'delivery', 'coupon',\n",
    "       #'postcode_invoice', 'postcode_delivery',\n",
    "       #'x_advertising_code_bin',  'goods_value', 'giftwrapping',\n",
    "       'referrer', 'cost_shipping', \n",
    "       'x_delivery_time_est', 'x_delivery_time', 'x_created_account', \n",
    "       #'weight', \n",
    "       'remitted_items', 'canceled_items',\n",
    "       #'used_items', \n",
    "       #'book_count', 'paperback_count', 'schoolbook_count', 'ebook_count', 'audiobook_count', 'audiobook_download_count', 'film_count', 'musical_count', 'hardware_count', 'imported_count', 'other_count'\n",
    "       ]\n",
    "\n",
    "# Load and split training  and testing data\n",
    "train = pd.read_csv(\"output/train_cleaned.csv\", sep=\";\", index_col=\"ID\")\n",
    "train = loadDataset(train)\n",
    "if use_full_feature_set:\n",
    "    features_to_use = train.columns.values.tolist()\n",
    "    features_to_use.remove(\"return_customer\")\n",
    "    for date_feature, v in train.dtypes.items():\n",
    "        if v == \"datetime64[ns]\": features_to_use.remove(date_feature)\n",
    "train = train[features_to_use + [\"return_customer\"]]\n",
    "train = pd.get_dummies(train)\n",
    "X_train, Y_train = train.drop(\"return_customer\", 1), train[\"return_customer\"]\n",
    "\n",
    "test = pd.read_csv(\"output/test_cleaned.csv\", sep=\";\", index_col=\"ID\")\n",
    "test = loadDataset(test)\n",
    "test = test[features_to_use]\n",
    "test = pd.get_dummies(test)\n",
    "test = test.reindex(columns = train.columns, fill_value=0)\n",
    "test.drop(\"return_customer\", 1, inplace=True)\n",
    "X_test = test\n",
    "\n",
    "# Oversample\n",
    "oversample = \"SMOTETomek\"\n",
    "if oversample == \"simple\":\n",
    "    ret_cust_idx = []\n",
    "    ret_cust_idx.extend(Y_train.loc[Y_train == 0].index)\n",
    "    ret_cust_idx.extend(Y_train.loc[Y_train == 1]\n",
    "                .sample(frac=(Y_train.loc[Y_train == 0].count() / Y_train.loc[Y_train == 1].count()),replace=True)\n",
    "                .index)\n",
    "    X_train, Y_train = X_train.loc[ret_cust_idx,:].reset_index(drop=True), Y_train[ret_cust_idx].reset_index(drop=True)\n",
    "elif oversample == \"SMOTE\":\n",
    "    sm = SMOTE(kind='regular')\n",
    "    X_train, Y_train = sm.fit_sample(X_train, Y_train)\n",
    "elif oversample == \"SMOTETomek\":\n",
    "    sm = SMOTETomek()\n",
    "    X_train, Y_train = sm.fit_sample(X_train, Y_train)\n",
    "    \n",
    "# Create Cross-Validation Splits\n",
    "X_train_cv, X_valid_cv, Y_train_cv, Y_valid_cv = train_test_split(X_train, Y_train, \n",
    "                                                      test_size = 0.15, random_state = 12345)\n",
    "                               \n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grid/Randomized Search Cross-Validation\n",
    "\n",
    "cost_func = bads_costs # bads_costs, roc_auc_score\n",
    "\n",
    "# Cross Validation\n",
    "rf_params = {'n_estimators':[100, 250], 'min_samples_split':[2, 4, 8], 'min_samples_leaf': [1, 3, 9]}\n",
    "clf_rf_cv = RandomizedSearchCV(RandomForestClassifier(), \n",
    "                         rf_params, \n",
    "                         scoring = make_scorer(cost_func), \n",
    "                         cv = 4)\n",
    "clf_rf_cv.fit(X_train_cv, Y_train_cv)\n",
    "#clf_rf_cv.cv_results_\n",
    "joblib.dump(clf_rf_cv.cv_results_, 'output/clf_rf_cv.results.pkl')\n",
    "print(\"Cross Valdiation Report:\")\n",
    "print(clf_rf_cv.best_params_)\n",
    "\n",
    "# Train and Validate a random forest classifier with the best parameters\n",
    "params_star = clf_rf_cv.best_params_\n",
    "clf_rf_star = RandomForestClassifier().set_params(**params_star)\n",
    "clf_rf_star.fit(X_train_cv, Y_train_cv)\n",
    "yhat_valid = clf_rf_star.predict(X_valid_cv)\n",
    "print(\"Validation Summary:\")\n",
    "print(cost_func(Y_valid_cv, yhat_valid))\n",
    "print(\"Validation: {} of {}\".format(np.sum(yhat_valid), len(yhat_valid)))\n",
    "print(confusion_matrix(Y_valid_cv, yhat_valid))\n",
    "\n",
    "# Train model with all data and use on the Test set\n",
    "clf_rf_star.fit(X_train, Y_train)\n",
    "yhat_test = clf_rf_star.predict(X_test)\n",
    "np.savetxt(\"output/test_return_customer.csv\", yhat_test.astype(int), fmt='%i', delimiter=\";\")\n",
    "print(\"Testing: {} of {}\".format(np.sum(yhat_test), len(yhat_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yhat_train = clf_rf_star.predict(train[features_to_use])\n",
    "print(\"Training: {} of {}\".format(np.sum(yhat_train), len(yhat_train)))\n",
    "print(confusion_matrix(train[\"return_customer\"], yhat_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Single Random Forest\n",
    "\n",
    "cost_func = roc_auc_score # bads_costs roc_auc_score\n",
    "\n",
    "clf_rf = RandomForestClassifier(n_estimators=4000)\n",
    "\n",
    "clf_rf.fit(X_train_cv, Y_train_cv)\n",
    "yhat_valid = clf_rf.predict(X_valid_cv)\n",
    "print(\"Validation Summary:\")\n",
    "print(cost_func(Y_valid_cv, yhat_valid))\n",
    "print(\"Validation: {} of {}\".format(np.sum(yhat_valid), len(yhat_valid)))\n",
    "print(confusion_matrix(Y_valid_cv, yhat_valid))\n",
    "\n",
    "#clf_rf.fit(X_train, Y_train)\n",
    "print(\"Test Results\")\n",
    "yhat = clf_rf.predict(X_test)\n",
    "print(\"{} of {}\".format(np.sum(yhat), len(yhat)))\n",
    "\n",
    "#clf_rf.fit(train[features_to_use], train[\"return_customer\"])\n",
    "#yhat = clf_rf.predict(test)\n",
    "#print(\"{} of {}\".format(np.sum(yhat), len(yhat)))\n",
    "#joblib.dump(clf_rf, 'output/model_rf.pkl')\n",
    "#clf_rf = joblib.load('output/model_rf.pkl')\n",
    "#cross_val_score(clf_rf, X_train, Y_train, scoring=make_scorer(cost_func), cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PCA Analysis\n",
    "\n",
    "num_PC = 5\n",
    "\n",
    "train_scaled = preprocessing.scale(X_train)\n",
    "pca = PCA(n_components=num_PC)\n",
    "pca.fit(train_scaled)\n",
    "train_rotated = pca.transform(train_scaled)\n",
    "df_train = pd.DataFrame(train_rotated)\n",
    "df_train[\"colors\"] = Y_train\n",
    "sns.pairplot(df_train, hue = \"colors\", diag_kind=\"kde\", vars=range(num_PC))\n",
    "plt.show()\n",
    "test_scaled = preprocessing.scale(test)\n",
    "test_rotated = pca.transform(test_scaled)\n",
    "df_test = pd.DataFrame(test_rotated)\n",
    "sns.pairplot(df_test, diag_kind=\"kde\", vars=range(num_PC))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gradient Boost Classifier (testing)\n",
    "\n",
    "cost_func = roc_auc_score # bads_costs roc_auc_score\n",
    "\n",
    "clf_gbc = GradientBoostingClassifier()\n",
    "clf_gbc.fit(X_train_cv, Y_train_cv)\n",
    "yhat_valid = clf_gbc.predict(X_valid_cv)\n",
    "print(\"Validation Summary:\")\n",
    "print(cost_func(Y_valid_cv, yhat_valid))\n",
    "print(\"Validation: {} of {}\".format(np.sum(yhat_valid), len(yhat_valid)))\n",
    "print(confusion_matrix(Y_valid_cv, yhat_valid))\n",
    "\n",
    "clf_gbc.fit(X_train, Y_train)\n",
    "print(\"Test Results\")\n",
    "yhat = clf_gbc.predict(X_test)\n",
    "print(\"{} of {}\".format(np.sum(yhat), len(yhat)))\n",
    "\n",
    "#cross_val_score(clf_gbc, X_train, Y_train, scoring=make_scorer(cost_func), cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order_date',\n",
       " 'account_creation_date',\n",
       " 'deliverydate_estimated',\n",
       " 'deliverydate_actual']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"output/train_cleaned.csv\", sep=\";\", index_col=\"ID\")\n",
    "train = loadDataset(train)\n",
    "[k for k,v in train.dtypes.items() if v == \"datetime64[ns]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
