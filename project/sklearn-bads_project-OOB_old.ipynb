{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, roc_auc_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BADS(object):\n",
    "    def __init__(self):\n",
    "        # Data\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_test = None\n",
    "        self.X_train_cv = None\n",
    "        self.X_valid_cv = None\n",
    "        self.y_train_cv = None\n",
    "        self.y_valid_cv = None\n",
    "        self.column_names = None\n",
    "        # Classifiers\n",
    "        self.clf = None\n",
    "        self.clf_cv = None\n",
    "        \n",
    "        # Cost matrix\n",
    "        self.cm = np.array([[3., 0.], [-10., 0.]])\n",
    "        \n",
    "        # variables to be set\n",
    "        self.rs = 90049\n",
    "        self.save_model = False\n",
    "\n",
    "        ######### Feature Selection #########\n",
    "        self.manual_features_to_remove = [\"item_count\", \"x_order_date_num\"]\n",
    "        self.feature_correlation_removal = False\n",
    "        self.feature_correlation_threshold = 0.7\n",
    "        self.automatic_feature_selection = False\n",
    "        self.automatic_feature_threshold = 0.005\n",
    "\n",
    "        ######### Oversampling #########\n",
    "        # non-standard package: http://contrib.scikit-learn.org/imbalanced-learn/index.html\n",
    "        self.oversample_method = \"none\"\n",
    "\n",
    "        ######### Cross-Valdiation #########\n",
    "        self.do_cv = False # this takes a long time\n",
    "        self.cv_num_folds = 4\n",
    "        self.cv_validation_frac = 0.15\n",
    "        self.cv_rs_iters = 20\n",
    "        self.cost_func = self.bads_costs # bads_costs, roc_auc_score\n",
    "        self.score_func = self.bads_scorer # bads_scorer, roc_auc_score\n",
    "        self.set_model(\"rf\") # \"rf\" or \"gbc\" or \"linear\"\n",
    "\n",
    "    def set_model(self, model_to_use):\n",
    "        self.model_to_use = model_to_use\n",
    "        ######### Model Selection #########\n",
    "        if model_to_use == \"rf\":\n",
    "            # Random Forest Classifier\n",
    "            from sklearn.ensemble import RandomForestClassifier\n",
    "            self.clf = RandomForestClassifier(random_state=self.rs)\n",
    "            self.automatic_feature_selection_params = {'n_estimators': 250, 'verbose': 0, 'n_jobs': 3}\n",
    "            self.clf_default_params = {'min_samples_split': 2, 'n_estimators': 250, \n",
    "                                       'min_samples_leaf': 9, 'verbose': 0, 'n_jobs': 3}\n",
    "            self.cv_param_grid = {'n_estimators':[100, 250, 500], \n",
    "                                  'min_samples_split':[2, 4, 8], \n",
    "                                  'min_samples_leaf': [1, 3, 9, 15], \n",
    "                                  'n_jobs': [3]}\n",
    "        elif model_to_use == \"gbc\":\n",
    "            # Gradient Boosting Classifier\n",
    "            from sklearn.ensemble import GradientBoostingClassifier\n",
    "            self.clf = GradientBoostingClassifier(random_state=self.rs)\n",
    "            self.automatic_feature_selection_params = {'n_estimators': 50, 'verbose': 1}\n",
    "            self.clf_default_params = {'learning_rate': 0.1, 'max_depth': 3, \n",
    "                                       'n_estimators': 100, 'verbose': 1}\n",
    "            self.cv_param_grid = {'n_estimators':[50, 100, 250, 500], \n",
    "                                  'learning_rate':[0.05, 0.1, .25], \n",
    "                                  'max_depth': [3, 5, 9]}\n",
    "        elif model_to_use == \"linear\":\n",
    "            # Logistic Regression Classifier\n",
    "            from sklearn import linear_model\n",
    "            self.clf = linear_model.LogisticRegression()\n",
    "            self.clf_default_params = {'penalty': 'l1'}\n",
    "            self.cv_param_grid = {'penalty':['l1', 'l2'], 'C': 2 ** np.linspace(-3, 5, 17), 'n_jobs': [3]}\n",
    "        else:\n",
    "            print(\"Please Set The Model\")\n",
    "\n",
    "    def loadDataset(self, df, date_to_int = True):\n",
    "        # remove NA\n",
    "        df.fillna(-99, inplace = True)\n",
    "        # Convert Dates\n",
    "        df.order_date = pd.to_datetime(df.order_date, format='%Y-%m-%d')\n",
    "        df.account_creation_date = pd.to_datetime(df.account_creation_date, format='%Y-%m-%d')\n",
    "        df.deliverydate_estimated = pd.to_datetime(df.deliverydate_estimated, format='%Y-%m-%d')\n",
    "        df.deliverydate_actual = pd.to_datetime(df.deliverydate_actual, format='%Y-%m-%d')\n",
    "        # Create weekday dummy for order_date\n",
    "        df['x_order_date_is_weekday'] = df.order_date.dt.dayofweek < 5\n",
    "        if date_to_int:\n",
    "            epoch_date = pd.Timestamp(\"2013-01-01\")\n",
    "            df.order_date = (df.order_date - epoch_date).astype('timedelta64[D]').astype(int)\n",
    "            df.account_creation_date = (df.account_creation_date - epoch_date).astype('timedelta64[D]').astype(int)\n",
    "            df.deliverydate_estimated = (df.deliverydate_estimated - epoch_date).astype('timedelta64[D]').astype(int)\n",
    "            df.deliverydate_actual = (df.deliverydate_actual - epoch_date).astype('timedelta64[D]').astype(int)\n",
    "        # Convert Categories (factors in R lingo)\n",
    "        cols_to_categorize = [\"model\", \"form_of_address\", \"email_domain\", \"postcode_invoice\", \"postcode_delivery\", \n",
    "                              \"payment\", \"advertising_code\", \"x_order_date_yearweek\"]\n",
    "        # Categorize _bin columns\n",
    "        cols = df.columns\n",
    "        cols_to_categorize.extend(cols[cols.str.contains(\"_bin\")].values.tolist())\n",
    "        for col_to_cat in cols_to_categorize:\n",
    "            #print(col_to_cat)\n",
    "            df[col_to_cat] = df[col_to_cat].astype('category')\n",
    "        return(df)\n",
    "\n",
    "    def simple_oversample_idx(self, y):\n",
    "        y_idx_0 = np.where(y == 0)[0]\n",
    "        y_idx_1 = np.random.choice(np.where(y == 1)[0], size=y_idx_0.shape[0], replace=True)\n",
    "        ret_cust_idx = []\n",
    "        ret_cust_idx.extend(y[y_idx_0])\n",
    "        ret_cust_idx.extend(y[y_idx_1])\n",
    "        return(ret_cust_idx)\n",
    "\n",
    "    def bads_costs(self, y_t, yhat):\n",
    "        N = yhat.shape[0]\n",
    "        C = confusion_matrix(y_t, yhat)\n",
    "        return(np.multiply(C, self.cm).sum() / N)\n",
    "\n",
    "    def bads_scorer(self, y_t, yhat_prob):\n",
    "        thresholds = np.linspace(0.01, 0.99)\n",
    "        costs = [self.bads_costs(y_t, yhat_prob[:,1] > threshold) for threshold in thresholds]\n",
    "        return(np.max(costs))\n",
    "\n",
    "    def find_corr_features(self, df, threshold = 0.7):\n",
    "        cols = df.columns.values.tolist()\n",
    "        corr_mat = df.corr()\n",
    "        corr_items = np.where(np.abs(np.triu(corr_mat, k=1)) > threshold)\n",
    "        cols_removed = []\n",
    "        for corr_item in list(set([cols[max(item)] for item in zip(*corr_items)])):\n",
    "            cols_removed.append(corr_item)\n",
    "            cols.remove(corr_item)\n",
    "        print(\"Removing Columns:\", \", \".join(cols_removed))\n",
    "        return(cols)\n",
    "\n",
    "    def create_datasets(self):\n",
    "        train = pd.read_csv(\"output/train_cleaned.csv\", sep=\";\", index_col=\"ID\")\n",
    "        train = self.loadDataset(train)\n",
    "        # Create Feature List\n",
    "        features_to_use = train.columns.values.tolist()\n",
    "        features_to_use.remove(\"return_customer\")\n",
    "        for ftr in self.manual_features_to_remove:\n",
    "            features_to_use.remove(ftr)\n",
    "        # remove dates if not converted to ints\n",
    "        for date_feature, v in train.dtypes.items():\n",
    "            if v == \"datetime64[ns]\": \n",
    "                features_to_use.remove(date_feature)\n",
    "        train = train[features_to_use + [\"return_customer\"]]\n",
    "        # Visualize Correlation before splitting out dummy variables\n",
    "        if self.feature_correlation_removal: \n",
    "            sns.heatmap(train.drop(\"return_customer\", 1).corr())\n",
    "            plt.show()\n",
    "        # Split out dummy variables    \n",
    "        train = pd.get_dummies(train)\n",
    "        # feature Correlation Removal\n",
    "        if self.feature_correlation_removal:\n",
    "            print(\"Removing correlated features...\")\n",
    "            noncorr_cols = find_corr_features(train.drop(\"return_customer\", 1), \n",
    "                                              self.feature_correlation_threshold)\n",
    "            train = train[noncorr_cols + [\"return_customer\"]]\n",
    "        # set train datasets\n",
    "        self.X_train, self.y_train = train.drop(\"return_customer\", 1).values, train[\"return_customer\"].values\n",
    "        self.column_names = train.columns\n",
    "\n",
    "        test = pd.read_csv(\"output/test_cleaned.csv\", sep=\";\", index_col=\"ID\")\n",
    "        test = self.loadDataset(test)\n",
    "        test = pd.get_dummies(test)\n",
    "        # The following line gives the test set the same columns as the training set. \n",
    "        # This simultaneously adds columns to the test set and sets the values in those columns to 0 and \n",
    "        # drops any columns in the test set that did not exist in the training set.\n",
    "        test = test.reindex(columns = self.column_names, fill_value=0)\n",
    "        test.drop(\"return_customer\", 1, inplace=True)\n",
    "        # set test dataset\n",
    "        self.X_test = test.values\n",
    "        self.X_train_cv, self.X_valid_cv, self.y_train_cv, self.y_valid_cv = train_test_split(self.X_train, \n",
    "                                                                                              self.y_train, \n",
    "                                                                                              test_size = self.cv_validation_frac,\n",
    "                                                                                              stratify = self.y_train,\n",
    "                                                                                              random_state = self.rs)\n",
    "\n",
    "\n",
    "    def oversample(self):\n",
    "        if self.oversample_method == \"simple\":\n",
    "            # oversampling with replacement of the minority group to equalize the size of the minority and \n",
    "            # majority group\n",
    "            print(\"Simple oversampling...\")\n",
    "            # Create the Hyper-Parameter Cross-Validation train and test sets\n",
    "            ret_cust_idx_cv = self.simple_oversample_idx(self.y_train_cv)\n",
    "            self.X_train_cv, self.y_train_cv = self.X_train_cv[ret_cust_idx_cv,:], self.y_train_cv[ret_cust_idx_cv]\n",
    "            # Create the full train and test sets\n",
    "            ret_cust_idx = simple_oversample_idx(self.y_train)\n",
    "            self.X_train, self.y_train = self.X_train[ret_cust_idx,:], self.y_train[ret_cust_idx]\n",
    "        elif self.oversample_method == \"SMOTE\":\n",
    "            # https://www.jair.org/media/953/live-953-2037-jair.pdf\n",
    "            from imblearn.over_sampling import SMOTE\n",
    "\n",
    "            print(\"SMOTE oversampling...\")\n",
    "            sm = SMOTE(kind='regular', random_state = self.rs)\n",
    "            # Create the Hyper-Parameter Cross-Validation train and test sets\n",
    "            self.X_train_cv, self.y_train_cv = sm.fit_sample(self.X_train_cv, self.y_train_cv)\n",
    "            # Create the full train and test sets\n",
    "            self.X_train, self.y_train = sm.fit_sample(self.X_train, self.y_train)\n",
    "        elif self.oversample_method == \"SMOTETomek\":\n",
    "            from imblearn.combine import SMOTETomek\n",
    "\n",
    "            print(\"SMOTE + Tomek Links oversampling...\")\n",
    "            sm = SMOTETomek(random_state = rs)\n",
    "            # Create the Hyper-Parameter Cross-Validation train and test sets\n",
    "            self.X_train_cv, self.y_train_cv = sm.fit_sample(self.X_train_cv, self.y_train_cv)\n",
    "            # Create the full train and test sets\n",
    "            self.X_train, self.y_train = sm.fit_sample(self.X_train, self.y_train)\n",
    "        else:\n",
    "            print(\"No oversampling...\")\n",
    "\n",
    "    def automagic_feature_selection(self):\n",
    "        if self.automatic_feature_selection:\n",
    "            print(\"Starting automatic feature selection...\")\n",
    "            # this takes about 10 minutes to run\n",
    "            self.clf.set_params(**self.automatic_feature_selection_params)\n",
    "            self.clf.fit(self.X_train, self.y_train)\n",
    "            important_features = np.where(self.clf.feature_importances_ > self.automatic_feature_threshold)[0].tolist()\n",
    "            important_features_labels = self.column_names[important_features]\n",
    "            print(\"High Importance Features:\", \", \".join(important_features_labels.tolist()))\n",
    "            np.savetxt(\"output/optimal_features.csv\", important_features_labels.values, fmt=\"%s\", delimiter=\";\")\n",
    "\n",
    "            self.X_train, self.X_test = self.X_train[:,important_features], self.X_test[:,important_features]\n",
    "            self.X_train_cv, self.X_valid_cv = self.X_train_cv[:,important_features], self.X_valid_cv[:,important_features]\n",
    "\n",
    "        else:\n",
    "            print(\"No automatic feature selection...\")\n",
    "\n",
    "    def run_model(self):\n",
    "        if self.do_cv:\n",
    "            # this can take a LONG time\n",
    "            print(\"Searching for best parameters with CV search...\")\n",
    "            self.clf_cv = RandomizedSearchCV(self.clf, self.cv_param_grid, \n",
    "                                             scoring = make_scorer(self.score_func, needs_proba=True), \n",
    "                                             cv = self.cv_num_folds, \n",
    "                                             n_iter = self.cv_rs_iters, \n",
    "                                             random_state = self.rs, verbose = 1)\n",
    "            self.clf_cv.fit(self.X_train_cv, self.y_train_cv)\n",
    "            #clf_rf_cv.cv_results_\n",
    "            joblib.dump(self.clf_cv.cv_results_, 'output/clf_rf_cv.results.pkl')\n",
    "            print(\"Cross Valdiation Report:\")\n",
    "            print(self.clf_cv.best_params_)\n",
    "            print(\"Best Score:\", self.clf_cv.best_score_)\n",
    "            # Plot Expected ROI per Customer\n",
    "            plt.errorbar(range(self.cv_rs_iters), \n",
    "                         self.clf_cv.cv_results_[\"mean_test_score\"], \n",
    "                         yerr = self.clf_cv.cv_results_[\"std_test_score\"], \n",
    "                         fmt=\"o\")\n",
    "            plt.margins(0.03)\n",
    "            plt.show()\n",
    "\n",
    "            # Train and Validate a random forest classifier with the best parameters\n",
    "            yhat_valid_prob = self.clf_cv.predict_proba(self.X_valid_cv)\n",
    "\n",
    "            params_star = self.clf_cv.best_params_\n",
    "            self.clf.set_params(**params_star)\n",
    "        else:\n",
    "            self.clf.set_params(**self.clf_default_params)\n",
    "            self.clf.fit(self.X_train_cv, self.y_train_cv)\n",
    "            yhat_valid_prob = self.clf.predict_proba(self.X_valid_cv)\n",
    "\n",
    "        print(\"Validation Summary:\")\n",
    "        print(\"Calculate Optimal Threshold\")\n",
    "        thresholds = np.linspace(0.01, 0.99, 197)\n",
    "        costs = [self.bads_costs(self.y_valid_cv, yhat_valid_prob[:,1] > threshold) for threshold in thresholds]\n",
    "        threshold_star = thresholds[np.argmax(costs)]\n",
    "        # Plot\n",
    "        plt.plot(thresholds, costs)\n",
    "        plt.show()\n",
    "        print(\"Threshold:\", threshold_star)\n",
    "        yhat_valid = yhat_valid_prob[:,1] > threshold_star\n",
    "        print(\"Average ROI:\", self.cost_func(self.y_valid_cv, yhat_valid))\n",
    "        print(\"ROC Score:\", roc_auc_score(self.y_valid_cv, yhat_valid_prob[:,1]))\n",
    "        print(\"Validation Return Customers: {} of {} ({}%)\".format(np.sum(yhat_valid), \n",
    "                                                                   len(yhat_valid), \n",
    "                                                                   np.sum(yhat_valid)/len(yhat_valid)))\n",
    "        print(confusion_matrix(self.y_valid_cv, yhat_valid))\n",
    "        # Train model with all data and use on the Test set\n",
    "        self.clf.fit(self.X_train, self.y_train)\n",
    "        yhat_test_proba = self.clf.predict_proba(self.X_test)\n",
    "        yhat_test = yhat_test_proba[:,1] > threshold_star\n",
    "        np.savetxt(\"output/test_return_customer.csv\", yhat_test.astype(int), fmt='%i', delimiter=\";\")\n",
    "        print(\"Testing Return Customers: {} of {} ({}%)\".format(np.sum(yhat_test), \n",
    "                                                                len(yhat_test), \n",
    "                                                                np.sum(yhat_test)/len(yhat_test)))\n",
    "\n",
    "        if self.save_model:\n",
    "            joblib.dump(self.clf, 'output/model_final.pkl')\n",
    "            #clf_rf = joblib.load('output/model_final.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "#\n",
    "#  Variable Selection\n",
    "#\n",
    "#####################################\n",
    "######### Set Seed #########\n",
    "rs = 90049\n",
    "save_model = False\n",
    "\n",
    "######### Feature Selection #########\n",
    "manual_features_to_remove = [\"item_count\", \"x_order_date_num\"]\n",
    "feature_correlation_removal = False\n",
    "feature_correlation_threshold = 0.7\n",
    "automatic_feature_selection = False\n",
    "automatic_feature_threshold = 0.005\n",
    "\n",
    "######### Oversampling #########\n",
    "# non-standard package: http://contrib.scikit-learn.org/imbalanced-learn/index.html\n",
    "oversample_method = \"none\"\n",
    "\n",
    "######### Cross-Valdiation #########\n",
    "do_cv = False # this takes a long time\n",
    "cv_num_folds = 4\n",
    "cv_validation_frac = 0.15\n",
    "cv_rs_iters = 10\n",
    "cost_func = bads_costs # bads_costs, roc_auc_score\n",
    "score_func = bads_scorer # bads_scorer, roc_auc_score\n",
    "#cost_func = roc_auc_score # bads_costs, roc_auc_score\n",
    "#score_func = roc_auc_score # bads_scorer, roc_auc_score\n",
    "\n",
    "######### Model Selection #########\n",
    "model_to_use = \"rf\" # \"rf\" or \"gbc\" or \"linear\"\n",
    "\n",
    "if model_to_use == \"rf\":\n",
    "    # Random Forest Classifier\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    clf = RandomForestClassifier(random_state=rs)\n",
    "    automatic_feature_selection_params = {'n_estimators': 250, 'verbose': 0, 'n_jobs': 3}\n",
    "    clf_default_params = {'min_samples_split': 2, 'n_estimators': 250, 'min_samples_leaf': 9, 'verbose': 0, 'n_jobs': 3}\n",
    "    cv_param_grid = {'n_estimators':[100, 250, 500], 'min_samples_split':[2, 4, 8], 'min_samples_leaf': [1, 3, 9], 'n_jobs': [3]}\n",
    "elif model_to_use == \"gbc\":\n",
    "    # Gradient Boosting Classifier\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    clf = GradientBoostingClassifier(random_state=rs)\n",
    "    automatic_feature_selection_params = {'n_estimators': 50, 'verbose': 1}\n",
    "    clf_default_params = {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'verbose': 1}\n",
    "    cv_param_grid = {'n_estimators':[50, 100, 250, 500], 'learning_rate':[0.05, 0.1, .25], 'max_depth': [3, 5, 9]}\n",
    "elif model_to_use == \"linear\":\n",
    "    # Logistic Regression Classifier\n",
    "    from sklearn import linear_model\n",
    "    clf = linear_model.LogisticRegression()\n",
    "    clf_default_params = {'penalty': 'l1'}\n",
    "    cv_param_grid = {'penalty':['l1', 'l2'], 'C': 2 ** np.linspace(-3, 5, 17), 'n_jobs': [3]}\n",
    "else:\n",
    "    print(\"Invalid Model Selected\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No oversampling...\n",
      "No automatic feature selection...\n",
      "(51884, 402) (12971, 402) (44101, 402) (7783, 402)\n",
      "Validation Summary:\n",
      "Calculate Optimal Threshold\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAECCAYAAAAb5qc/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njhfEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4360c6e1d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.225\n",
      "Average ROI: 0.844404471284\n",
      "ROC Score: 0.662009236429\n",
      "Validation Return Customers: 2169 of 7783 (0.2786843119619684%)\n",
      "[[4824 1493]\n",
      " [ 790  676]]\n",
      "Testing Return Customers: 3582 of 12971 (0.27615449849664636%)\n"
     ]
    }
   ],
   "source": [
    "a = BADS()\n",
    "# Load and split training  and testing data and create cross validation sets from training data\n",
    "a.create_datasets()\n",
    "# Oversample if desired\n",
    "a.oversample()\n",
    "# Run algorithm-based feature selection\n",
    "a.automagic_feature_selection()\n",
    "# Print size of training sets\n",
    "print(a.X_train.shape, a.X_test.shape, a.X_train_cv.shape, a.X_valid_cv.shape)\n",
    "# Change a few variables as a test\n",
    "a.set_model(\"rf\")\n",
    "a.do_cv = False\n",
    "# Run the models\n",
    "a.run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PCA Analysis of Results\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "num_PC = 5\n",
    "\n",
    "train_scaled = scale(X_train)\n",
    "pca = PCA(n_components=num_PC)\n",
    "pca.fit(train_scaled)\n",
    "train_rotated = pca.transform(train_scaled)\n",
    "df_train = pd.DataFrame(train_rotated)\n",
    "df_train[\"colors\"] = y_train\n",
    "sns.pairplot(df_train, hue = \"colors\", diag_kind=\"kde\", vars=range(num_PC))\n",
    "plt.show()\n",
    "test_scaled = scale(X_test)\n",
    "test_rotated = pca.transform(test_scaled)\n",
    "df_test = pd.DataFrame(test_rotated)\n",
    "df_test[\"colors\"] = yhat_test\n",
    "sns.pairplot(df_test, hue = \"colors\", diag_kind=\"kde\", vars=range(num_PC))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tests Below This Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Single Random Forest\n",
    "\n",
    "cost_func = roc_auc_score # bads_costs roc_auc_score\n",
    "\n",
    "clf_rf = RandomForestClassifier(n_estimators=4000)\n",
    "\n",
    "clf_rf.fit(X_train_cv, y_train_cv)\n",
    "yhat_valid = clf_rf.predict(X_valid_cv)\n",
    "print(\"Validation Summary:\")\n",
    "print(cost_func(y_valid_cv, yhat_valid))\n",
    "print(\"Validation: {} of {} ({}%)\".format(np.sum(yhat_valid), len(yhat_valid), np.sum(yhat_valid)/len(yhat_valid)))\n",
    "print(confusion_matrix(y_valid_cv, yhat_valid))\n",
    "\n",
    "clf_rf.fit(X_train, y_train)\n",
    "print(\"Test Results\")\n",
    "yhat = clf_rf.predict(X_test)\n",
    "print(\"{} of {}\".format(np.sum(yhat), len(yhat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gradient Boost Classifier (testing)\n",
    "\n",
    "cost_func = roc_auc_score # bads_costs roc_auc_score\n",
    "\n",
    "clf_gbc = GradientBoostingClassifier(learning_rate=0.05, n_estimators=500, random_state=1234, verbose=1)\n",
    "clf_gbc.fit(X_train_cv, y_train_cv)\n",
    "yhat_valid = clf_gbc.predict(X_valid_cv)\n",
    "print(\"Validation Summary:\")\n",
    "print(cost_func(y_valid_cv, yhat_valid))\n",
    "print(\"Validation: {} of {}\".format(np.sum(yhat_valid), len(yhat_valid)))\n",
    "print(confusion_matrix(y_valid_cv, yhat_valid))\n",
    "print(X_train.columns[np.where(clf_gbc.feature_importances_ > 0.005)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample Confusion Matrix\n",
    "\n",
    "a = np.array([1, 0, 0, 1, 1, 1, 0, 0, 0, 0])\n",
    "b = np.array([True, False, False, False, False, False, True, True, True, True])\n",
    "confusion_matrix(a,b), bads_costs(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(range(10), clf_cv.cv_results_[\"mean_test_score\"], yerr = clf_cv.cv_results_[\"std_test_score\"], fmt=\"o\")\n",
    "plt.margins(0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Results\n",
    "\n",
    "### Gradient Boosting: SMOTE+Tomek, 500 iterations, .1 eta, 5 max_depth, no decorrellation, no feature selection, no hyper-parameter search\n",
    "\n",
    "Time: ~45m\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.29\n",
    "0.772966722344\n",
    "Validation Return Customers: 1193 of 7783\n",
    "[[5532  785]\n",
    " [1058  408]]\n",
    "Testing Return Customers: 1478 of 12971\n",
    "\n",
    "### Gradient Boosting: SMOTE+Tomek, 50 iterations, .25 eta, 5 max_depth, no decorrellation, no feature selection, no hyper-parameter search\n",
    "\n",
    "Time: ~5m; iter: 50; training loss: 0.5494\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.23\n",
    "0.775664910703\n",
    "Validation Return Customers: 2239 of 7783\n",
    "[[4729 1588]\n",
    " [ 815  651]]\n",
    "Testing Return Customers: 3694 of 12971\n",
    "\n",
    "### Gradient Boosting: SMOTE+Tomek, 50 iterations, .25 eta, 12 max_depth, no decorrellation, no feature selection, no hyper-parameter search\n",
    "\n",
    "Time: 30.77m; iter: 50; training loss: 0.3084\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.25\n",
    "0.687909546447\n",
    "Validation Return Customers: 1613 of 7783\n",
    "[[5158 1159]\n",
    " [1012  454]]\n",
    "Testing Return Customers: did not complete\n",
    "\n",
    "### Gradient Boosting: No Oversampling, 50 iterations, .25 eta, 12 max_depth, no decorrellation, no feature selection, no hyper-parameter search\n",
    "\n",
    "Time: 23.24m; iter: 50; training loss: 0.4498\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.17\n",
    "0.67814467429\n",
    "Validation Return Customers: 2687 of 7783\n",
    "[[4326 1991]\n",
    " [ 770  696]]\n",
    "Testing Return Customers: 3321 of 12971\n",
    "\n",
    "### Gradient Boosting: No Oversampling, 50 iterations, .15 eta, 3 max_depth, no decorrellation, no feature selection, no hyper-parameter search\n",
    "\n",
    "Time: 1.20m; iter: 50; training loss: 0.9077\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.21\n",
    "0.812925607092\n",
    "Validation Return Customers: 2294 of 7783\n",
    "[[4709 1608]\n",
    " [ 780  686]]\n",
    "Testing Return Customers: 3924 of 12971\n",
    "\n",
    "### Gradient Boosting: No Oversampling, 50 iterations, .15 eta, 3 max_depth, decorrelation, feature selection, no hyper-parameter search\n",
    "\n",
    "Time: 4.51m; iter: 50; training loss: 0.9096\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.21\n",
    "0.824617756649\n",
    "Validation Return Customers: 2385 of 7783\n",
    "[[4646 1671]\n",
    " [ 752  714]]\n",
    "Testing Return Customers: 4003 of 12971\n",
    "\n",
    "### Gradient Boosting: No Oversampling, 100 iterations, .05 eta, 3 max_depth, decorrelation, feature selection, no hyper-parameter search\n",
    "\n",
    "Time: 7.99m; iter: 100; training loss: 0.9159\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.23\n",
    "0.809970448413\n",
    "Validation Return Customers: 1669 of 7783\n",
    "[[5188 1129]\n",
    " [ 926  540]]\n",
    "Testing Return Customers: 2815 of 12971\n",
    "\n",
    "---\n",
    "\n",
    "### RandomForest: No Oversampling, 500 trees, min sample split 4, min leaf size 9, decorr, gb feature selection, no hyper-parameter search\n",
    "\n",
    "Time: ~2m\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.23\n",
    "0.800848002056\n",
    "Validation Return Customers: 2178 of 7783\n",
    "[[4791 1526]\n",
    " [ 814  652]]\n",
    "Testing Return Customers: 3700 of 12971\n",
    "\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "### RandomForest: No Oversampling, All Bin to Cat, 500 trees, min sample split 4, min leaf size 9, no decorr, no feature selection, no hyper-parameter search\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.24\n",
    "Average ROI: 0.837851728125\n",
    "ROC Score: 0.662306189517\n",
    "Validation Return Customers: 1705 of 7783 (0.21906719773866118%)\n",
    "[[5177 1140]\n",
    " [ 901  565]]\n",
    "Testing Return Customers: 2801 of 12971 (0.21594325803715983%)\n",
    "\n",
    "\n",
    "### RandomForest: No Oversampling, All Bin to Cat, 500 trees, min sample split 4, min leaf size 9, decorr, rf feature selection, no hyper-parameter search\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.23\n",
    "Average ROI: 0.788513426699\n",
    "ROC Score: 0.647177725452\n",
    "Validation Return Customers: 2184 of 7783 (0.28061158936142877%)\n",
    "[[4779 1538]\n",
    " [ 820  646]]\n",
    "Testing Return Customers: 4028 of 12971 (0.31053889445686533%)\n",
    "\n",
    "### RandomForest: SMOTE Oversampling, All Bin to Cat, 500 trees, min sample split 4, min leaf size 9, decorr, rf feature selection, no hyper-parameter search\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.27\n",
    "Average ROI: 0.796479506617\n",
    "ROC Score: 0.64603845143\n",
    "Validation Return Customers: 2016 of 7783 (0.2590260824874727%)\n",
    "[[4913 1404]\n",
    " [ 854  612]]\n",
    "Testing Return Customers: 8841 of 12971 (0.6815974096060442%)\n",
    "\n",
    "### Logistic Regression: no Oversampling, L1 Penalty, C = 0.5, no decorr, no feature selection, rand hyper-parameter search\n",
    "\n",
    "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n",
    "Done  40 out of  40 | elapsed: 27.8min finished\n",
    "\n",
    "{'n_jobs': 3, 'C': 0.5, 'penalty': 'l1'}\n",
    "Best Score: 0.794607832022\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.23\n",
    "Average ROI: 0.783374020301\n",
    "ROC Score: 0.649006146605\n",
    "Validation Return Customers: 1998 of 7783 (0.25671334960812026%)\n",
    "[[4919 1398]\n",
    " [ 866  600]]\n",
    "Testing Return Customers: 3266 of 12971 (0.25179246010330736%)\n",
    "\n",
    "### Random Forest: no Oversampling, min_sample_leaf = 9, n_estimators = 250, min_samples_split = 2, no decorr, no feature selection, rand hyper-parameter search\n",
    "\n",
    "Fitting 4 folds for each of 10 candidates, totalling 40 fits\n",
    "Done  40 out of  40 | elapsed: 71.8min finished\n",
    "Cross Valdiation Report:\n",
    "{'n_jobs': 3, 'min_samples_leaf': 9, 'n_estimators': 250, 'min_samples_split': 2}\n",
    "Best Score: 0.811478197773\n",
    "\n",
    "Validation Summary:\n",
    "Calculate Optimal Threshold\n",
    "\n",
    "Threshold: 0.235\n",
    "Average ROI: 0.833097777207\n",
    "ROC Score: 0.662557465822\n",
    "Validation Return Customers: 1856 of 7783 (0.23846845689322882%)\n",
    "[[5058 1259]\n",
    " [ 869  597]]\n",
    "Testing Return Customers: 3057 of 12971 (0.23567959293809268%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RandomizedSearchCV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = np.array([[3., 0.], [-10., 0.]])\n",
    "cm0 = confusion_matrix(train.return_customer, np.zeros(train.return_customer.count()))\n",
    "cm1 = confusion_matrix(train.return_customer, np.ones(train.return_customer.count()))\n",
    "costs = np.multiply(cm0, m)\n",
    "print(cm0);print(m);print(costs);print(costs.sum()/train.return_customer.count());print(np.multiply(cm1, m).sum()/train.return_customer.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50333758430304909"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.set_model(\"linear\")\n",
    "a.do_cv = True\n",
    "a.run_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
